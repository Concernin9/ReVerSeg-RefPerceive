# ğŸŒŸ ReVerSeg-RefPerceive  
**Training-Free Reason-and-Verify Framework for Language-Driven Traffic Video Segmentation**  

ğŸš— **Traffic Scene Understanding** &nbsp;|&nbsp; ğŸ¯ **Language-Driven Segmentation** &nbsp;|&nbsp; ğŸ§  **Training-Free Inference**

---

<p align="center">
  <img src="assets/teaser.png" width="90%">
</p>

> **ReVerSeg** introduces a training-free *Reason-and-Verify* inference framework for robust language-driven traffic video segmentation.  
> We also present **Ref-Perceive**, a large-scale traffic-oriented benchmark with compositional referring instructions and frame-wise pixel annotations.

ğŸ“„ Paper: *Under Review (IEEE T-ITS)*  
ğŸ“¦ Dataset & Code: **Will be released after paper acceptance**

---

## âœ¨ Highlights

- âœ… **Training-Free Inference Pipeline**  
  No fine-tuning. No additional data. Plug-and-play with foundation models.

- ğŸ” **Semantic-Aware Keyframe Selection**  
  Automatically selects the most reliable frame for localization instead of using a fixed anchor.

- ğŸ§ª **Two-Stage Verification Mechanism**  
  - Bounding-box level semantic verification  
  - Mask-level consistency verification  
  Effectively suppresses error propagation and identity drift.

- ğŸŒ **Traffic-Oriented Benchmark (Ref-Perceive)**  
  Covers multi-city, multi-weather, multi-illumination real-world traffic scenarios.

---

## ğŸ§  Method Overview: ReVerSeg

<p align="center">
  <img src="assets/pipeline.png" width="95%">
</p>

### ğŸ” Training-Free Reason-and-Verify Pipeline

Compared with the traditional localization â†’ propagation pipeline, ReVerSeg introduces structured inference-time control:

**Stage 1 â€” Semantic Keyframe Selection**  
Selects the most semantically reliable frame for target grounding.

**Stage 2-1 â€” Localization Verification**  
Checks whether predicted bounding boxes truly match the referring instruction.

**Stage 2-2 â€” Segmentation Verification**  
Validates mask-level semantic consistency to refine pixel predictions.

This design improves robustness **without modifying any foundation model parameters**.

---

## ğŸ“Š Ref-Perceive Dataset

<p align="center">
  <img src="assets/dataset_overview.png" width="90%">
</p>

### ğŸ“¦ Dataset Summary

| Property | Value |
|---------|-------|
| Video Clips | **2,000** |
| Frames | **10,000** |
| Frames per Clip | **5** |
| Annotation Type | Pixel-level masks |
| Instruction Type | Compositional referring expressions |
| Domain | Real-world traffic scenes |

---

### ğŸŒ Geographic & Environmental Diversity

<p align="center">
  <img src="assets/world_map.png" width="85%">
</p>

Data collected across:

- ğŸ‡ºğŸ‡¸ New York, Chicago, Hollywood  
- ğŸ‡¬ğŸ‡§ London  
- ğŸ‡¸ğŸ‡¬ Singapore  
- ğŸ‡¨ğŸ‡³ Harbin  

Conditions include:

- â˜€ï¸ Sunny  
- ğŸŒ§ Rainy  
- ğŸŒ™ Night  
- â„ï¸ Snow  

---

### ğŸ“ Instruction Design

Ref-Perceive adopts **compositional referring expressions**, combining:

- Appearance  
- Spatial location  
- Object relations  
- Temporal consistency  
- Motion / state  

Example:

> *"Segment the white sedan driving in the center lane throughout all frames of the video."*

This design enforces **instance-level reasoning** and **cross-frame identity consistency**.

---

## ğŸ§ª Experimental Results

<p align="center">
  <img src="assets/results.png" width="95%">
</p>

### ğŸš€ Performance Highlights

ReVerSeg consistently outperforms both:

- Image-based referring segmentation methods (GLAMM, LISA, UFO, PixelLM)
- Video-based baselines (VideoLISA, VISA, GLUS, VRS-HQ)

While preserving:

- Zero-shot generalization  
- Training-free deployment  
- Backbone-agnostic compatibility (Qwen2 / Qwen3 / InternVL)

---

## ğŸ“ Code & Dataset Release Plan

âš ï¸ **Important Notice**

This repository currently serves as the **official project homepage for peer review**.

- ğŸ“¦ Dataset: **Will be released after paper acceptance**
- ğŸ’» Code: **Will be released after paper acceptance**
- ğŸ“‘ Evaluation scripts and pretrained configurations will be provided together

We strictly follow journal data and code release policies.
